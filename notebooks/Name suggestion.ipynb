{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip3 install coloredlogs youtokentome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "* Extract function definitions\n",
    "* Highlight names and identifiers in function\n",
    "* Features and labels extraction\n",
    "* Train BPE\n",
    "* Train seq2seq model\n",
    "* Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "import coloredlogs\n",
    "\n",
    "from utils import DirsABC, FilesABC, Run, SUPPORTED_LANGUAGES, query_gitbase\n",
    "\n",
    "from enum import Enum\n",
    "from os import makedirs\n",
    "from os.path import join as path_join\n",
    "from typing import Union\n",
    "\n",
    "class Files(FilesABC, Enum):\n",
    "    FUNCTIONS = [\"functions.jsonl.bz2\"]\n",
    "    FUNC_ID_NAME = [\"functions_identifers_names.pkl.bz2\"]\n",
    "    BPE_MODEL = [\"bpe.model\"]\n",
    "    BPE_INPUT = [\"bpe_input.txt\"]\n",
    "    TRAIN_BODIES = [\"train.src\"]    \n",
    "    TRAIN_NAMES = [\"train.tgt\"]\n",
    "    VAL_BODIES = [\"val.src\"]\n",
    "    VAL_NAMES = [\"val.tgt\"]\n",
    "    ENC_TRAIN_BODIES = [\"train.bpe.src\"]\n",
    "    ENC_TRAIN_NAMES = [\"train.bpe.tgt\"]\n",
    "    ENC_VAL_BODIES = [\"val.bpe.src\"]\n",
    "    ENC_VAL_NAMES = [\"val.bpe.tgt\"]\n",
    "    TGT_VOCABULARY = [\"tgt.vocab\"]\n",
    "    SRC_VOCABULARY = [\"src.vocab\"]\n",
    "\n",
    "    \n",
    "class Dirs(DirsABC, Enum):\n",
    "    TF_MODELS = [\"tf\", \"models\"]\n",
    "\n",
    "run = Run(\"name-suggestion\", \"java-full\")\n",
    "\n",
    "coloredlogs.install(level=\"WARNING\")\n",
    "logging.getLogger(\"matplotlib.axes._base\").setLevel(logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gitbase\n",
    "\n",
    "### What is Gitbase?\n",
    "* it is **SQL** interface to git repositories\n",
    "* refrasing: it allows to **query** and **retrieve** required information from **code**\n",
    "\n",
    "### In our case it will help with:\n",
    "* language classification of files\n",
    "* selecting files from specific programming language\n",
    "* filtering out vendor & binary files\n",
    "* parsing files - we don't work with code as raw text - we extract Unified Abstract Syntax Trees (**UAST**)\n",
    "* extracting function-related parts of **UAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from bz2 import open as bz2_open\n",
    "from json import dumps as json_dumps, loads as json_loads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_function_group(functions_path: str, limit: int = 0):    \n",
    "    sql = \"\"\"SELECT\n",
    "        files.repository_id as repository_id,\n",
    "        files.file_path as path,\n",
    "        files.blob_content as content,\n",
    "        UAST(files.blob_content, LANGUAGE(files.file_path, files.blob_content), '//uast:FunctionGroup') as functions\n",
    "    FROM files\n",
    "    NATURAL JOIN commit_files\n",
    "    NATURAL JOIN commits\n",
    "    NATURAL JOIN refs\n",
    "    WHERE\n",
    "        refs.ref_name= 'HEAD' and functions IS NOT NULL\n",
    "        AND LANGUAGE(files.file_path, files.blob_content) = 'Java'\n",
    "        AND NOT IS_VENDOR(file_path)\n",
    "        AND NOT IS_BINARY(file_path)\n",
    "    %s\n",
    "    \"\"\" % ( \"LIMIT %d\" % limit if limit > 0 else \"\" )\n",
    "    with bz2_open(functions_path, \"wt\", encoding=\"utf8\") as fh:\n",
    "        for row in query_gitbase(sql):\n",
    "            row[\"content\"] = base64.b64encode(row[\"content\"]).decode(\"utf-8\")\n",
    "            row[\"functions\"] = base64.b64encode(row[\"functions\"]).decode(\"utf-8\")\n",
    "            fh.write(\"%s\\n\" % json_dumps(row))\n",
    "\n",
    "\n",
    "extract_function_group(run.path(Files.FUNCTIONS), 3)\n",
    "\n",
    "#repositories, paths, contents, function_groups = run(function_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of function groups\", len(function_groups)) # 21374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract function names and identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function_name(function_node):\n",
    "    func_name, func_name_pos = None, None\n",
    "    for node in function_node[\"Nodes\"]:\n",
    "        if node is None or \"@type\" not in node:\n",
    "            continue\n",
    "        if node[\"@type\"] == 'uast:Alias':\n",
    "            func_name = node[\"Name\"][\"Name\"]\n",
    "            func_name_pos = (node[\"Name\"][\"@pos\"][\"start\"][\"offset\"], node[\"Name\"][\"@pos\"][\"end\"][\"offset\"])\n",
    "    return func_name, func_name_pos\n",
    "\n",
    "\n",
    "def get_identifiers(node):\n",
    "    if (isinstance(node, dict) and \n",
    "        '@type' in node and \n",
    "        node['@type']  == 'uast:Identifier'):\n",
    "        yield node[\"Name\"], (node[\"@pos\"][\"start\"][\"offset\"], node[\"@pos\"][\"end\"][\"offset\"])\n",
    "    else:\n",
    "        if isinstance(node, dict):\n",
    "            for k in node:\n",
    "                yield from get_identifiers(node[k])\n",
    "        elif isinstance(node, list) or isinstance(node, tuple):\n",
    "            for n in node:\n",
    "                yield from get_identifiers(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "Inspect what is going to be a model feature and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import colored_text_by_pos, Colored, RED, GREEN\n",
    "from bblfsh.pyuast import decode as uast_decode\n",
    "\n",
    "def highlight_function_name_and_identifiers(functions_path: str, limit: int = 0):\n",
    "    with bz2_open(functions_path, \"rt\", encoding=\"utf8\") as fh_functions:\n",
    "        processed = 0\n",
    "        for row_str in fh_functions:\n",
    "            row = json_loads(row_str)\n",
    "            content = base64.b64decode(row[\"content\"]).decode('utf-8', 'replace')\n",
    "            func_group = uast_decode(base64.b64decode(row[\"functions\"]), format=0).load()\n",
    "\n",
    "            for func in func_group:\n",
    "                if limit > 0 and processed >= limit:\n",
    "                    return\n",
    "                processed += 1\n",
    "                identifiers = None\n",
    "                for node in func[\"Nodes\"]:        \n",
    "                    if node is None or \"@type\" not in node:\n",
    "                        continue\n",
    "                    if node[\"@type\"] == 'uast:Alias':\n",
    "                        func_body = node[\"Node\"][\"Body\"]\n",
    "                        body_identifiers = sorted(get_identifiers(func_body), key=lambda x: x[1][0])\n",
    "                func_name = get_function_name(func)\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "                start_offset = func[\"@pos\"][\"start\"][\"offset\"]\n",
    "                end_offset = func[\"@pos\"][\"end\"][\"offset\"]\n",
    "                colored_texts = []\n",
    "                colored_texts.append(Colored(color=RED, position=func_name[1], start_offset=start_offset))\n",
    "                for bi in body_identifiers:\n",
    "                    colored_texts.append(Colored(color=GREEN, position=bi[1], start_offset=start_offset))\n",
    "                \n",
    "                print(colored_text_by_pos(content[start_offset:end_offset], colored_texts))\n",
    "\n",
    "\n",
    "highlight_function_name_and_identifiers(run.path(Files.FUNCTIONS), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and labels extraction\n",
    "\n",
    "Prepare raw model input: \n",
    " - X identifiers from the body,\n",
    " - Y lable, a name of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_functions_parallel(functions_path: str, limit: int = 0):\n",
    "\n",
    "    def read_function_group(functions_path: str, limit: int = 0):\n",
    "        with bz2_open(functions_path, \"rt\", encoding=\"utf-8\") as fh_functions:\n",
    "            processed = 0\n",
    "            for row_str in fh_functions:\n",
    "                row = json_loads(row_str)\n",
    "                func_group = base64.b64decode(row[\"functions\"])\n",
    "                if limit > 0 and processed >= limit:\n",
    "                    break\n",
    "                processed += 1\n",
    "                yield func_group\n",
    "\n",
    "\n",
    "    def process_function_group(func_group):\n",
    "        res = []\n",
    "        try:\n",
    "            func_group = uast_decode(func_group, format=0).load()\n",
    "\n",
    "            for func in func_group:\n",
    "                body_identifiers = None        \n",
    "                for node in func[\"Nodes\"]:        \n",
    "                    if node is None or \"@type\" not in node:\n",
    "                        continue\n",
    "                    if node[\"@type\"] == 'uast:Alias':\n",
    "                        func_body = node[\"Node\"][\"Body\"]\n",
    "                        body_identifiers = sorted(get_identifiers(func_body), key=lambda x: x[1][0])\n",
    "                if body_identifiers is None:\n",
    "                    continue\n",
    "                res.append(([i[0] for i in body_identifiers], get_function_name(func)[0]))\n",
    "        except:\n",
    "            print(\"decoding error\")\n",
    "        return res\n",
    "\n",
    "    function_group_res = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_function_group)(fg) for fg in read_function_group(run.path(Files.FUNCTIONS), limit))\n",
    "    \n",
    "    res = itertools.chain.from_iterable(function_group_res)\n",
    "    deduplicated_res = filter(lambda x: x[0], set(map(lambda x: (tuple(x[0]), x[1]), res)))\n",
    "    \n",
    "    df = pd.DataFrame(deduplicated_res, columns = ['function_identifiers', 'function_name'])\n",
    "    df.to_pickle(run.path(Files.FUNC_ID_NAME))\n",
    "    del(df)\n",
    "\n",
    "\n",
    "extract_functions_parallel(run.path(Files.FUNCTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BPE\n",
    "\n",
    "In order to feed text data into the model (identifers) we need to represent it in the vector form.\n",
    "\n",
    "There are multiple ways to do so:\n",
    " 1. **word level**, assign a uniq number for every identifer\n",
    "    * *pro*: easy to implement (hashtable)\n",
    "    * *con*: huge (and rapidly growing) vocabulary size\n",
    "    * *con*: how to deal with Out Of Vacabulary (OOV) tokens? E.g by replace with \"-UNK-\"\n",
    " 2. **char level**, assign a uniq number for every char\n",
    "    * *pro*: small vocabulary size\n",
    "    * *pro*: no OOV\n",
    "    * *con*: model need to \"learn\" much more, e.g. to compose words first :/ \n",
    " 3. **sub-word level**, assign a uniq number for every sub-word (based on frequency)\n",
    "    * *pro*: small vocabulary size (hyperparameter)\n",
    "    * *pro*: easty to deal with OOV\n",
    "    * *con*: additional \"training\" step, harder to implement\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare BPE training data\n",
    "\n",
    "We are going to use a sing vocabulary for both, identifiers and function names. In order to do so, we will need to train BPE tokenizer on a file that contains all identifiers and function names in plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LANG=en_US.UTF-8\n",
    "%env LC_ALL=en_US.UTF-8\n",
    "\n",
    "def prepare_bpe_text_file(pkl_functions_path: str, text_file_path: str, limit: int = 0):\n",
    "    df = pd.read_pickle(pkl_functions_path)\n",
    "    with open(text_file_path, \"wt\") as text_file_fh:\n",
    "        for i, row in df.iterrows():\n",
    "            if limit > 0 and i >= limit:\n",
    "                break\n",
    "            text_file_fh.write(\" \".join(row[\"function_identifiers\"]))\n",
    "            text_file_fh.write(\" {}\\n\".format(row[\"function_name\"]))\n",
    "\n",
    "prepare_bpe_text_file(run.path(Files.FUNC_ID_NAME), run.path(Files.BPE_INPUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BPE tokenizer\n",
    "\n",
    "There are multile BPE algorithm impelementaitons, here we are going to use optimized C++ one form https://github.com/VKCOM/YouTokenToMe using its CLI interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "!yttm bpe --data {run.path(Files.BPE_INPUT)} --model {run.path(Files.BPE_MODEL)} --vocab_size {vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset on Training / Vaidation\n",
    "\n",
    "Create a dataset for training and a separate, holdout one for validation using handy [`model_selection` scikit-learn helper](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_pickle(run.path(Files.FUNC_ID_NAME))\n",
    "train_bodies, val_bodies, train_names, val_names = train_test_split(df.function_identifiers, df.function_name, \n",
    "                                                                    test_size=0.1, random_state=1989)\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset splits\n",
    "\n",
    "In the plain text format, suitable for further processing by OpenNMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def save_as_text(output_path: str, data: pd.Series):\n",
    "    with open(output_path, \"wt\") as output_fd:\n",
    "        for row in tqdm(data):\n",
    "            if isinstance(row, str):\n",
    "                output_fd.write(row + \"\\n\")\n",
    "            else:\n",
    "                output_fd.write(\" \".join(row) + \"\\n\")\n",
    "            \n",
    "save_as_text(run.path(Files.TRAIN_BODIES), train_bodies)\n",
    "save_as_text(run.path(Files.TRAIN_NAMES), train_names)\n",
    "save_as_text(run.path(Files.VAL_BODIES), val_bodies)\n",
    "save_as_text(run.path(Files.VAL_NAMES), val_names)\n",
    "\n",
    "del(train_bodies); del(val_bodies); del(train_names); del(val_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode dataset\n",
    "\n",
    "Get vector represenation using the vocabulary from the trained BPE tokenizer, in the format compatible with [OpenNMT](http://opennmt.net/OpenNMT-tf/data.html#vocabulary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the vocabulary on disk\n",
    "\n",
    "We'll need only one file, as the same vocabulary will be used for both, identifiers and function names. Different vocabularies can be used without any change to the model e.g the sub-words (BPE) only for identifers and char for the function names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run.path(Files.VOCABULARY), \"w\") as vocab_fd:\n",
    "    for i in range(vocab_size + 5):\n",
    "        vocab_fd.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode dataset splits using BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtokentome as yttm\n",
    "\n",
    "bpe = yttm.BPE(model=run.path(Files.BPE_MODEL))\n",
    "\n",
    "def bpe_encode(input_path: str, output_path: str):\n",
    "    with open(output_path, \"w\") as output_fd, open(input_path, \"rt\") as input_fd:\n",
    "        data = input_fd.readlines()\n",
    "        for row in bpe.encode(data, output_type=yttm.OutputType.ID):\n",
    "            output_fd.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "bpe_encode(run.path(Files.TRAIN_BODIES), run.path(Files.ENC_TRAIN_BODIES))\n",
    "bpe_encode(run.path(Files.TRAIN_NAMES), run.path(Files.ENC_TRAIN_NAMES))\n",
    "bpe_encode(run.path(Files.VAL_BODIES), run.path(Files.ENC_VAL_BODIES))\n",
    "bpe_encode(run.path(Files.VAL_BODIES), run.path(Files.ENC_VAL_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train seq2seq model\n",
    "\n",
    "* we will use `openNMT-tf`\n",
    "* prepare vocabularies (we will use functionality to train translation model from identifiers to function names)\n",
    "* train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: src_vocab_loc, tgt_vocab_loc\n",
    "\n",
    "# approach requires to provide vocabularies\n",
    "# so launch these commands\n",
    "def generate_build_vocab(save_vocab_loc, input_text, vocab_size=vocab_size):\n",
    "    return \"onmt-build-vocab --size %s --save_vocab %s %s\" % (vocab_size, \n",
    "                                                              save_vocab_loc,\n",
    "                                                              input_text)\n",
    "\n",
    "src_vocab_loc = os.path.join(bpe_base_dir, \"src.vocab\")\n",
    "print(generate_build_vocab(save_vocab_loc=src_vocab_loc,\n",
    "                           input_text=train_bodies_bpe_loc,\n",
    "                           vocab_size=vocab_size + 10))\n",
    "tgt_vocab_loc = os.path.join(bpe_base_dir, \"tgt.vocab\")\n",
    "print(generate_build_vocab(save_vocab_loc=tgt_vocab_loc,\n",
    "                           input_text=train_names_bpe_loc,\n",
    "                           vocab_size=vocab_size + 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_dir =  os.path.join(bpe_base_dir, \"seq2seq\")\n",
    "os.makedirs(base_train_dir, exist_ok=True)\n",
    "model_dir = os.path.join(base_train_dir, \"run/\")\n",
    "\n",
    "# prepare config file for model\n",
    "config_yaml = os.path.join(base_train_dir, \"config.yml\")\n",
    "# this directory will contain evaluation results of the model, checkpoints and so on\n",
    "yaml_content = \"model_dir: %s \\n\" % model_dir\n",
    "\n",
    "# describe where data is located\n",
    "yaml_content += \"\"\"\n",
    "data:\n",
    "  train_features_file: %s\n",
    "  train_labels_file: %s\n",
    "  eval_features_file: %s\n",
    "  eval_labels_file: %s\n",
    "  source_vocabulary: %s\n",
    "  target_vocabulary: %s\n",
    "\"\"\" % (train_bodies_bpe_loc, train_names_bpe_loc,\n",
    "       val_bodies_bpe_loc, val_names_bpe_loc,\n",
    "       src_vocab_loc, tgt_vocab_loc)\n",
    "\n",
    "# other useful configurations\n",
    "yaml_content += \"\"\"\n",
    "train:\n",
    "  # (optional when batch_type=tokens) If not set, the training will search the largest\n",
    "  # possible batch size.\n",
    "  batch_size: 256\n",
    "\n",
    "eval:\n",
    "  # (optional) The batch size to use (default: 32).\n",
    "  batch_size: 128\n",
    "\n",
    "  # (optional) Evaluate every this many steps (default: 5000).\n",
    "  steps: 5000\n",
    "\n",
    "  # (optional) Save evaluation predictions in model_dir/eval/.\n",
    "  save_eval_predictions: false\n",
    "  # (optional) Evalutator or list of evaluators that are called on the saved evaluation predictions.\n",
    "  # Available evaluators: bleu, rouge\n",
    "  external_evaluators: bleu\n",
    "\n",
    "  # (optional) Export a SavedModel when a metric has the best value so far (default: null).\n",
    "  export_on_best: bleu\n",
    "\n",
    "  # (optional) Early stopping condition.\n",
    "  # Should be read as: stop the training if \"metric\" did not improve more\n",
    "  # than \"min_improvement\" in the last \"steps\" evaluations.\n",
    "  early_stopping:\n",
    "    # (optional) The target metric name (default: \"loss\").\n",
    "    metric: bleu\n",
    "    # (optional) The metric should improve at least by this much to be considered as an improvement (default: 0)\n",
    "    min_improvement: 0.01\n",
    "    steps: 2\n",
    "\"\"\"\n",
    "\n",
    "with open(config_yaml, \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to launch training\n",
    "train_cmd = \"\"\"\n",
    "onmt-main --model_type LuongAttention \\\n",
    "--config %s --auto_config train --with_eval\"\"\" % config_yaml\n",
    "print(train_cmd)\n",
    "\n",
    "# in case of GPU you can specify CUDA_VISIBLE_DEVICES & number of GPUs to use\n",
    "cmd_gpu = \"\"\"\n",
    "CUDA_VISIBLE_DEVICES=%s onmt-main --model_type LuongAttention \\\n",
    "--config %s --auto_config train --with_eval --num_gpus %s\"\"\" % (\"0,1\", config_yaml, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alex.ipynb\t\t      jupyter-server-config.json  requirements-tf.txt\r\n",
      "base_egor.ipynb\t\t      Makefile\t\t\t  requirements.txt\r\n",
      "Dockerfile\t\t      notebooks\t\t\t  src.vocab\r\n",
      "docs\t\t\t      pretrained.zip\t\t  sshuttle.pid\r\n",
      "images\t\t\t      README.md\t\t\t  tgt.vocab\r\n",
      "jupyter-notebook-config.json  requirements-bigartm.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "* we will use pretrained on several GPUs model to save time\n",
    "* predictions will be saved to file \n",
    "* predicted BPE ids will be converted back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_val_predictions = os.path.join(base_dir, \"val.pred.tgt\")\n",
    "pretrained_model = os.path.join(base_dir, \"pretrained/model\")\n",
    "predict_cmd = \"\"\"onmt-main \\\n",
    "--config %s --auto_config \\\n",
    "infer \\\n",
    "--features_file %s \\\n",
    "--predictions_file %s \\\n",
    "--checkpoint_path run/baseline/avg/ckpt-5000\"\"\" % (config_yaml, val_bodies_bpe_loc, bpe_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = []\n",
    "with open(bpe_val_predictions, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        pred_ids.append(list(map(int, line.split())))\n",
    "\n",
    "pred_val_function_names = bpe.decode(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ids = []\n",
    "with open(val_names_bpe_loc, \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        gt_ids.append(list(map(int, line.split())))\n",
    "gt_val_function_names = bpe.decode(gt_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And finally let's see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (a, b) in enumerate(zip(gt_function_names, predicted_function_names)):\n",
    "    if i == 100:\n",
    "        break\n",
    "    print(\"%s | %s\" % (a, b))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
